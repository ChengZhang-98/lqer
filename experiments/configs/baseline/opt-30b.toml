project="template_baseline"
enable_wandb=false
enable_perplexity_evaluation=true
enable_harness_downstream_evaluation=true
tags=["template", "baseline"]
model_name="facebook/opt-30b"
task="causal_lm"
overwrite_checkpoint=true

[wandb]
    project="lqer-baselines"
    job_type="opt-30b"
    tags=["opt-30b"]

[evaluate]
    evaluate_baseline=true
    hf_quant_method="llm_int4"
    device_map=":ast:{'model.decoder.embed_tokens': 0, 'model.decoder.embed_positions': 0, 'model.decoder.final_layer_norm': 0, 'model.decoder.layers.0': 0, 'model.decoder.layers.1': 0, 'model.decoder.layers.2': 0, 'model.decoder.layers.3': 1, 'model.decoder.layers.4': 1, 'model.decoder.layers.5': 1, 'model.decoder.layers.6': 1, 'model.decoder.layers.7': 1, 'model.decoder.layers.8': 1, 'model.decoder.layers.9': 2, 'model.decoder.layers.10': 2, 'model.decoder.layers.11': 2, 'model.decoder.layers.12': 2, 'model.decoder.layers.13': 2, 'model.decoder.layers.14': 2, 'model.decoder.layers.15': 3, 'model.decoder.layers.16': 3, 'model.decoder.layers.17': 3, 'model.decoder.layers.18': 3, 'model.decoder.layers.19': 3, 'model.decoder.layers.20': 3, 'model.decoder.layers.21': 4, 'model.decoder.layers.22': 4, 'model.decoder.layers.23': 4, 'model.decoder.layers.24': 4, 'model.decoder.layers.25': 4, 'model.decoder.layers.26': 4, 'model.decoder.layers.27': 5, 'model.decoder.layers.28': 5, 'model.decoder.layers.29': 5, 'model.decoder.layers.30': 5, 'model.decoder.layers.31': 5, 'model.decoder.layers.32': 5, 'model.decoder.layers.33': 6, 'model.decoder.layers.34': 6, 'model.decoder.layers.35': 6, 'model.decoder.layers.36': 6, 'model.decoder.layers.37': 6, 'model.decoder.layers.38': 6, 'model.decoder.layers.39': 7, 'model.decoder.layers.40': 7, 'model.decoder.layers.41': 7, 'model.decoder.layers.42': 7, 'model.decoder.layers.43': 7, 'model.decoder.layers.44': 7, 'model.decoder.layers.45': 7, 'model.decoder.layers.46': 7, 'model.decoder.layers.47': 7, 'lm_head': 0}"
    [evaluate.perplexity]
        dataset="wikitext2"
        batch_size=2
        num_workers=8
    [evaluate.harness_downstream]
        no_cache=true
        datasets=["arc_easy", "lambada_openai", "piqa", "arc_challenge", "boolq", "openbookqa"]
        batch_size=16
