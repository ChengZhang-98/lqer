project="lqer_act_template"
enable_wandb=false
enable_profiling=true
enable_approximation=true
enable_perplexity_evaluation=true
enable_harness_downstream_evaluation=true

checkpoint_path="../../checkpoints/template/llama-30b"
tags=["template"]
model_name="huggyllama/llama-30b"

[wandb]
    project="lqer-act"
    job_type="llama-30b"
    tags=["llama-30b"]

[evaluate]
    disable_lqer=false
    low_rank_dict="../../checkpoints/template/llama-30b/approximate/low_rank_dict.pt"
    [evaluate.perplexity]
        dataset="wikitext2"
        batch_size=2
        num_workers=8
        # GPU x 8
        # device_map=":ast:{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 4, 'model.layers.32': 4, 'model.layers.33': 4, 'model.layers.34': 4, 'model.layers.35': 4, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 5, 'model.layers.40': 5, 'model.layers.41': 5, 'model.layers.42': 5, 'model.layers.43': 5, 'model.layers.44': 5, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 6, 'model.layers.48': 6, 'model.layers.49': 6, 'model.layers.50': 6, 'model.layers.51': 6, 'model.layers.52': 6, 'model.layers.53': 6, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 7, 'model.layers.57': 7, 'model.layers.58': 7, 'model.layers.59': 7, 'model.norm': 7, 'lm_head': 7}"
        # GPU x 3
        device_map=":ast:{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.norm': 2, 'lm_head': 2}"
    [evaluate.harness_downstream]
        # GPU x 8
        # device_map=":ast:{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 4, 'model.layers.32': 4, 'model.layers.33': 4, 'model.layers.34': 4, 'model.layers.35': 4, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 5, 'model.layers.40': 5, 'model.layers.41': 5, 'model.layers.42': 5, 'model.layers.43': 5, 'model.layers.44': 5, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 6, 'model.layers.48': 6, 'model.layers.49': 6, 'model.layers.50': 6, 'model.layers.51': 6, 'model.layers.52': 6, 'model.layers.53': 6, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 7, 'model.layers.57': 7, 'model.layers.58': 7, 'model.layers.59': 7, 'model.norm': 7, 'lm_head': 7}"
        # GPU x 3
        device_map=":ast:{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.norm': 2, 'lm_head': 2}"
        batch_size=8
        datasets=["arc_easy", "copa", "lambada_openai", "piqa", "winogrande", "arc_challenge", "boolq", "openbookqa"]
        # datasets=["lambada_openai", "arc_easy"]

[profile]
    dataset="slim_pajama_6b"
    max_length=2048
    batch_size=2
    num_workers=8
    num_samples=64
    num_raw_samples=4096
    scale_dict="../../checkpoints/template/llama-30b/profile/scale_dict.pt"
    # GPU x 8
    # device_map=":ast:{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 4, 'model.layers.32': 4, 'model.layers.33': 4, 'model.layers.34': 4, 'model.layers.35': 4, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 5, 'model.layers.40': 5, 'model.layers.41': 5, 'model.layers.42': 5, 'model.layers.43': 5, 'model.layers.44': 5, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 6, 'model.layers.48': 6, 'model.layers.49': 6, 'model.layers.50': 6, 'model.layers.51': 6, 'model.layers.52': 6, 'model.layers.53': 6, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 7, 'model.layers.57': 7, 'model.layers.58': 7, 'model.layers.59': 7, 'model.norm': 7, 'lm_head': 7}"
    # GPU x 3
    device_map=":ast:{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.norm': 2, 'lm_head': 2}"

[approximate]
    name="lqer-act"
    device="cuda"
    chunk_size=16
    chunk_idx=0
    [approximate.lqer_svd]
    [approximate.lqer_act]
    [approximate.approximator]
        # 'default' means use the entry named default
        'model\.layers\.[0-9]+\.self_attn\.(k|q|v|o)_proj\.weight'="default"
        'model\.layers\.[0-9]+\.mlp\.(gate|down|up)_proj\.weight'="default"
        [approximate.approximator.default]
            rank=128
            [approximate.approximator.default.W_quantizer]
                name="block_fp"
                width=4
                exponent_width=8
                exponent_bias="NA"
                block_size=[1, 16]
                skip_first_dim=false
            [approximate.approximator.default.A_quantizer]
                name="block_fp"
                width=8
                exponent_width=8
                exponent_bias="NA"
                block_size=[16, 1]
                skip_first_dim=false
            [approximate.approximator.default.B_quantizer]
                name="block_fp"
                width=8
                exponent_width=8
                exponent_bias="NA"
                block_size=[16, 1]
                skip_first_dim=false
# this should be the same as the ranks in the approximator
[l_config.linear]
    rank=32

[q_config.linear]
    name="flexible_lqer"
    is_ptq=true
    default=false
    [q_config.linear.x_quantizer]
        name="block_fp"
        width=8
        exponent_width=8
        exponent_bias="NA"
        block_size=[1, 16]
        skip_first_dim=true

    [q_config.linear.w_quantizer]
        # this should be the same as the entry in the approximator
        name="block_fp"
        width=4
        exponent_width=8
        exponent_bias="NA"
        block_size=[-1, -1]
        skip_first_dim=false

    [q_config.linear.b_quantizer]
        name="block_fp"
        width=8
        exponent_width=8
        exponent_bias="NA"
        block_size=[-1]
        skip_first_dim=false

# omit this entry, LinearFlexibleLqer will use the q_config of x_quantizer
# same rule applies to B_out_quantizer
# [q_config.linear.A_out_quantizer]
[q_config.matmul]
    name="flexible"
    default=false
    [q_config.matmul.x_quantizer]
        name="block_fp"
        width=8
        exponent_width=8
        exponent_bias="NA"
        block_size=[1, 16]
        skip_first_dim=true
    [q_config.matmul.w_quantizer]
        name="block_fp"
        width=8
        exponent_width=8
        exponent_bias="NA"
        block_size=[1, 16]
        skip_first_dim=true

[q_config.bmm]
    name="flexible"
    default=false
    [q_config.bmm.x_quantizer]
        name="block_fp"
        width=8
        exponent_width=8
        exponent_bias="NA"
        block_size=[1, 16]
        skip_first_dim=true
    [q_config.bmm.w_quantizer]
        name="block_fp"
        width=8
        exponent_width=8
        exponent_bias="NA"
        block_size=[1, 16]
        skip_first_dim=true
